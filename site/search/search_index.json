{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"ImageCaption Documentation","text":""},{"location":"index.html#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index.html#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml \ndocs/\n    index.md \n    sources/\n        processFeatures.md\n        processTrain.md\n        dataManager.md\n        common/\n            common.md\n            paramsManager.md\n            utils.md\n</code></pre>"},{"location":"sources/dataManager.html","title":"data Management and storage","text":""},{"location":"sources/dataManager.html#sources.dataManager","title":"<code>sources.dataManager</code>","text":""},{"location":"sources/dataManager.html#sources.dataManager.saveModel","title":"<code>saveModel(model, type)</code>","text":"<p>Save the model to a specified file based on its type.</p> <p>This function saves the trained model to a file depending on the specified type. It supports saving LightGBM models as <code>.pkl</code> files and PyTorch models as <code>.pth</code> files. If an error occurs during the saving process, it raises an exception.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The trained model to be saved.</p> required <code>type</code> <code>str</code> <p>The type of the model, which determines the file format to be used.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path where the model was saved.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the model saving process.</p> Source code in <code>sources/dataManager.py</code> <pre><code>def saveModel(model, type):\n    \"\"\"\n    Save the model to a specified file based on its type.\n\n    This function saves the trained model to a file depending on the specified type. It supports saving LightGBM models\n    as `.pkl` files and PyTorch models as `.pth` files. If an error occurs during the saving process, it raises an exception.\n\n    :param model: The trained model to be saved.\n    :type model: object\n    :param type: The type of the model, which determines the file format to be used.\n    :type type: str\n\n    :return: The path where the model was saved.\n    :rtype: str\n\n    :raises Exception: If an error occurs during the model saving process.\n    \"\"\"\n    try:\n        if type == \"lightgbm\":\n            modelPath = os.path.join(processControl.env['models'], \"lightgbm_model.pkl\")\n            joblib.dump(model, modelPath)\n\n        if type == \"features\":\n            modelPath = os.path.join(processControl.env['outputPath'], \"features.pth\")\n            torch.save(model, modelPath)\n\n    except Exception as e:\n        raise Exception(f\"Couldn't save model: {e}\")\n\n    log_(\"info\", logger, f\"Model type: {type} saved to {modelPath}\")\n    return modelPath\n</code></pre>"},{"location":"sources/processFeatures.html","title":"Process Image Features and build clusters","text":""},{"location":"sources/processFeatures.html#sources.processFeatures","title":"<code>sources.processFeatures</code>","text":""},{"location":"sources/processFeatures.html#sources.processFeatures.buildLabels","title":"<code>buildLabels(clustered_images)</code>","text":"<p>Build a mapping of image names to their corresponding cluster labels.</p> <p>This function takes a dictionary of clustered images, where each key is a cluster label and the corresponding value is a list of images in that cluster. It then creates a mapping of image names to their respective cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>clustered_images</code> <code>dict</code> <p>A dictionary mapping cluster labels to lists of image names belonging to those clusters.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping image names to their corresponding cluster labels.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def buildLabels(clustered_images):\n    \"\"\"\n    Build a mapping of image names to their corresponding cluster labels.\n\n    This function takes a dictionary of clustered images, where each key is a cluster label and\n    the corresponding value is a list of images in that cluster. It then creates a mapping of image names\n    to their respective cluster labels.\n\n    :param clustered_images: A dictionary mapping cluster labels to lists of image names belonging to those clusters.\n    :type clustered_images: dict\n\n    :return: A dictionary mapping image names to their corresponding cluster labels.\n    :rtype: dict\n    \"\"\"\n    labels = {}\n    for cluster_label, images in clustered_images.items():\n        for image in images:\n            labels[image] = cluster_label\n    return labels\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.clusterImages","title":"<code>clusterImages(featuresFile)</code>","text":"<p>Perform clustering on image features to group images based on similarity.</p> <p>This function loads precomputed image features from a specified file, applies PCA for dimensionality reduction, and then clusters the images using KMeans. The images are grouped into clusters based on their feature vectors.</p> <p>Parameters:</p> Name Type Description Default <code>featuresFile</code> <code>str</code> <p>Path to the file containing saved image features.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - clustered_images (dict): A dictionary mapping cluster labels to lists of image names. - centroids (numpy.ndarray): The cluster centers after the KMeans clustering.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def clusterImages(featuresFile):\n    \"\"\"\n    Perform clustering on image features to group images based on similarity.\n\n    This function loads precomputed image features from a specified file, applies PCA for dimensionality reduction,\n    and then clusters the images using KMeans. The images are grouped into clusters based on their feature vectors.\n\n    :param featuresFile: Path to the file containing saved image features.\n    :type featuresFile: str\n\n    :return: A tuple containing:\n        - clustered_images (dict): A dictionary mapping cluster labels to lists of image names.\n        - centroids (numpy.ndarray): The cluster centers after the KMeans clustering.\n    :rtype: tuple\n    \"\"\"\n    # Load saved features\n    # weights_only=True se puede incluir para evitar warnings dado que solo queremos cargar los pesos del modelo\n    image_features = torch.load(featuresFile, weights_only=True)\n\n    # Convert feature tensors to a matrix for clustering\n    feature_matrix = torch.stack(list(image_features.values())).numpy()\n\n    # Dimensionality reduction (optional)\n    pca = PCA(n_components=processControl.defaults['features'])  # Reduce dimensions\n    reduced_features = pca.fit_transform(feature_matrix)\n    joblib.dump(pca, os.path.join(processControl.env['models'], \"pca_transform.pkl\"))\n    # Clustering\n    num_clusters = len(processControl.defaults['imageClasses'])\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(reduced_features)  # Train clustering\n    cluster_labels = kmeans.predict(reduced_features)\n\n    centroids = kmeans.cluster_centers_\n\n    # Assign images to clusters\n    clustered_images = {i: [] for i in range(num_clusters)}\n    for idx, image_name in enumerate(image_features.keys()):\n        clustered_images[cluster_labels[idx]].append(image_name)\n\n    # Print clusters\n    for cluster, images in clustered_images.items():\n        log_(\"info\", logger, f\"Clustering {len(images)} images\")\n\n    return clustered_images, centroids\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.extractFeaturesForInference","title":"<code>extractFeaturesForInference(imageFolder)</code>","text":"<p>Extract features from images in a specified folder for inference.</p> <p>This function extracts features from images stored in a folder, typically used for inference. The extracted features are returned as a numpy array, which can then be processed or used for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>imageFolder</code> <code>str</code> <p>Path to the folder containing images for which features are to be extracted.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array containing the extracted features for each image in the folder.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def extractFeaturesForInference(imageFolder):\n    \"\"\"\n    Extract features from images in a specified folder for inference.\n\n    This function extracts features from images stored in a folder, typically used for inference. The extracted features\n    are returned as a numpy array, which can then be processed or used for prediction.\n\n    :param imageFolder: Path to the folder containing images for which features are to be extracted.\n    :type imageFolder: str\n\n    :return: A numpy array containing the extracted features for each image in the folder.\n    :rtype: numpy.ndarray\n    \"\"\"\n    features = featureExtract(imageFolder)\n    image_features = []\n    for feature in features.values():\n        image_features.append(feature)\n    return np.array(image_features)\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.featureExtract","title":"<code>featureExtract(imageFolder, device='cuda' if torch.cuda.is_available() else 'cpu')</code>","text":"<p>Extract features from images in a specified folder using a pre-trained model.</p> <p>This function processes the images in the specified folder and extracts their features using a pre-trained model. The extracted features are returned as a dictionary where the keys are image names and the values are the feature vectors.</p> <p>Parameters:</p> Name Type Description Default <code>imageFolder</code> <code>str</code> <p>Path to the folder containing images from which features will be extracted.</p> required <code>device</code> <code>str, optional</code> <p>The device on which the model will run, either 'cuda' for GPU or 'cpu'. Defaults to 'cuda' if available.</p> <code>'cuda' if is_available() else 'cpu'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the extracted features for each image, with the image names as keys.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def featureExtract(imageFolder, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"\n    Extract features from images in a specified folder using a pre-trained model.\n\n    This function processes the images in the specified folder and extracts their features using a pre-trained model.\n    The extracted features are returned as a dictionary where the keys are image names and the values are the feature vectors.\n\n    :param imageFolder: Path to the folder containing images from which features will be extracted.\n    :type imageFolder: str\n    :param device: The device on which the model will run, either 'cuda' for GPU or 'cpu'. Defaults to 'cuda' if available.\n    :type device: str, optional\n\n    :return: A dictionary containing the extracted features for each image, with the image names as keys.\n    :rtype: dict\n    \"\"\"\n    def load_model():\n        model, preprocess, _ = open_clip.create_model_and_transforms(\n            model_name=processControl.process['modelName'],\n            pretrained=processControl.process['pretrainedDataset']\n        )\n        model.eval()  # Set model to evaluation mode\n        return model, preprocess\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, preprocess = load_model()\n    model.to(device)\n\n    image_features = {}\n    for image_name in tqdm(os.listdir(imageFolder), desc=\"Extracting features\"):\n        image_path = os.path.join(processControl.env['inputPath'], image_name)\n        if not os.path.exists(image_path):\n            log_(\"error\", logger, f\"Image {image_name} not found.\")\n            continue  # Skip missing images\n        try:\n            image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n            with torch.no_grad():\n                features = model.encode_image(image).squeeze(0).cpu()  # Move to CPU for storage\n            image_features[image_name] = features\n        except Exception as e:\n            log_(\"exception\", logger, f\"Error processing {image_name}: {e}\")\n\n    return image_features\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.optimizeDimensions","title":"<code>optimizeDimensions(image_features)</code>","text":"<p>Optimize the dimensionality of image features using PCA.</p> <p>This function applies Principal Component Analysis (PCA) to reduce the dimensionality of the extracted image features while retaining the most important variance. It computes the cumulative explained variance and plots it to help select the optimal number of PCA components that capture a desired level of variance (e.g., 95%).</p> <p>:note: The number of PCA components is dynamically chosen to capture at least 95% of the explained variance.</p> <p>Parameters:</p> Name Type Description Default <code>image_features</code> <code>dict</code> <p>A dictionary mapping image names to their corresponding feature tensors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping image names to their corresponding reduced feature arrays.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def optimizeDimensions(image_features):\n    \"\"\"\n    Optimize the dimensionality of image features using PCA.\n\n    This function applies Principal Component Analysis (PCA) to reduce the dimensionality\n    of the extracted image features while retaining the most important variance. It computes\n    the cumulative explained variance and plots it to help select the optimal number of PCA components\n    that capture a desired level of variance (e.g., 95%).\n\n    :param image_features: A dictionary mapping image names to their corresponding feature tensors.\n    :type image_features: dict\n\n    :return: A dictionary mapping image names to their corresponding reduced feature arrays.\n    :rtype: dict\n\n    :note: The number of PCA components is dynamically chosen to capture at least 95% of the explained variance.\n    \"\"\"\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n\n    feature_matrix = np.array([tensor.numpy() for tensor in image_features.values()])\n    image_names = list(image_features.keys())\n    # Initialize PCA without specifying n_components to analyze variance\n    pca = PCA()\n    pca.fit(feature_matrix)\n\n    # Compute cumulative explained variance\n    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n    # Plot variance to choose an optimal number of components\n    plt.plot(range(1, len(explained_variance) + 1), explained_variance)\n    plt.xlabel('Number of PCA Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Explained Variance by PCA Components')\n    plt.grid()\n    plt.show()\n\n    # Choose n_components dynamically, e.g., for 95% explained variance\n    optimal_components = np.argmax(explained_variance &gt;= 0.95) + 1\n    log_(\"info\", logger, f\"Optimal number of PCA components: {optimal_components}\")\n    # Apply PCA with the optimal number of components\n    pca = PCA(n_components=optimal_components)\n    reduced_features = pca.fit_transform(feature_matrix)\n    # \u2705 Convert reduced feature array back into a dictionary\n    reduced_feature_dict = {image_names[i]: reduced_features[i] for i in range(len(image_names))}\n\n    return reduced_feature_dict\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.processFeatures","title":"<code>processFeatures()</code>","text":"<p>Extract, optimize, cluster, and organize image features.</p> <p>This function performs the following steps: 1. Extracts features from images in the specified input directory. 2. Optimizes the dimensionality of the extracted features using PCA. 3. Saves the extracted features to a file. 4. Clusters the images based on their features. 5. Organizes the images into directories based on their cluster assignments. 6. Builds a mapping of images to their respective cluster labels.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - featuresFile (str): The path to the file containing the extracted image features. - imagesLabels (dict): A dictionary mapping image names to their corresponding cluster labels.</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def processFeatures():\n    \"\"\"\n    Extract, optimize, cluster, and organize image features.\n\n    This function performs the following steps:\n    1. Extracts features from images in the specified input directory.\n    2. Optimizes the dimensionality of the extracted features using PCA.\n    3. Saves the extracted features to a file.\n    4. Clusters the images based on their features.\n    5. Organizes the images into directories based on their cluster assignments.\n    6. Builds a mapping of images to their respective cluster labels.\n\n    :return: A tuple containing:\n        - featuresFile (str): The path to the file containing the extracted image features.\n        - imagesLabels (dict): A dictionary mapping image names to their corresponding cluster labels.\n    :rtype: tuple\n    \"\"\"\n    # Step 1: Extract features from images in the input directory\n    imageFeatures = featureExtract(processControl.env['inputPath'])\n\n    # Step 2: Optimize the dimensionality of the extracted features using PCA\n    imageFeatures2 = optimizeDimensions(imageFeatures)\n\n    # Step 3: Save the extracted features to a file\n    featuresFile = saveModel(imageFeatures, \"features\")\n\n    # Step 4: Cluster the images based on their features\n    clusteredImages, centroids = clusterImages(featuresFile)\n\n    # Step 5: Organize the images into directories based on their clusters\n    structureFiles(clusteredImages)\n\n    # Step 6: Build a mapping of images to their cluster labels\n    imagesLabels = buildLabels(clusteredImages)\n\n    # Return the path to the features file and the image-to-label mapping\n    return featuresFile, imagesLabels\n</code></pre>"},{"location":"sources/processFeatures.html#sources.processFeatures.structureFiles","title":"<code>structureFiles(clustered_images)</code>","text":"<p>Organize images into directories based on their cluster labels.</p> <p>This function takes a dictionary of clustered images, where each key is a cluster label and the corresponding value is a list of image names. It creates directories named by cluster labels and moves the images into the appropriate directories.</p> <p>Parameters:</p> Name Type Description Default <code>clustered_images</code> <code>dict</code> <p>A dictionary mapping cluster labels to lists of image names belonging to those clusters.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>sources/processFeatures.py</code> <pre><code>def structureFiles(clustered_images):\n    \"\"\"\n    Organize images into directories based on their cluster labels.\n\n    This function takes a dictionary of clustered images, where each key is a cluster label and\n    the corresponding value is a list of image names. It creates directories named by cluster labels and\n    moves the images into the appropriate directories.\n\n    :param clustered_images: A dictionary mapping cluster labels to lists of image names belonging to those clusters.\n    :type clustered_images: dict\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    for index, images in clustered_images.items():\n        # Create directory name\n        dir_name = os.path.join(processControl.env['outputPath'], f\"images_{index}\")\n\n        # Create the directory if it doesn't exist\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n\n        # Move images to the directory\n        for image in images:\n            # Assuming images are in the current working directory\n            imgSource = os.path.join(processControl.env['inputPath'], image)\n            if os.path.exists(imgSource):\n                shutil.copy(imgSource, os.path.join(dir_name, image))\n            else:\n                log_(\"error\", logger, f\"Image {image} not found.\")\n\n    log_(\"info\", logger, f\"Images organized into directories.\")\n</code></pre>"},{"location":"sources/processTrain.html","title":"process Train and Apply (inference) of Models","text":""},{"location":"sources/processTrain.html#sources.processTrain","title":"<code>sources.processTrain</code>","text":""},{"location":"sources/processTrain.html#sources.processTrain.loadFeatureslabels","title":"<code>loadFeatureslabels(features_file, labels)</code>","text":"<p>Load image features and corresponding labels from a file.</p> <p>This function loads precomputed image features from a specified file and matches them with their corresponding labels.</p> <p>Parameters:</p> Name Type Description Default <code>features_file</code> <code>str</code> <p>Path to the file containing saved image features.</p> required <code>labels</code> <code>dict</code> <p>A dictionary mapping image names to their corresponding labels.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two lists: - X (list of numpy.ndarray): The list of feature arrays for each image. - y (list): The list of labels corresponding to each image.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def loadFeatureslabels(features_file, labels):\n    \"\"\"\n    Load image features and corresponding labels from a file.\n\n    This function loads precomputed image features from a specified file and matches them with their corresponding labels.\n\n    :param features_file: Path to the file containing saved image features.\n    :type features_file: str\n    :param labels: A dictionary mapping image names to their corresponding labels.\n    :type labels: dict\n\n    :return: A tuple containing two lists:\n        - X (list of numpy.ndarray): The list of feature arrays for each image.\n        - y (list): The list of labels corresponding to each image.\n    :rtype: tuple\n    \"\"\"\n    # image_features = torch.load(features_file)\n    # weights_only=True se puede incluir para evitar warnings dado que solo queremos cargar los pesos del modelo\n    image_features = torch.load(features_file, weights_only=True)\n    X, y = [], []\n    for image_name, features in image_features.items():\n        if image_name in labels:  # Ensure label exists for the image\n            X.append(features.numpy())  # Convert tensor to numpy array\n            y.append(labels[image_name])\n    return X, y\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.predictLightgbm","title":"<code>predictLightgbm(imageFolder, imageList)</code>","text":"<p>Predict the labels for a list of images using a pre-trained LightGBM model.</p> <p>This function loads a pre-trained LightGBM model and PCA transformation, extracts features for the new images, applies PCA, and makes predictions using the loaded model.</p> <p>Parameters:</p> Name Type Description Default <code>imageFolder</code> <code>str</code> <p>Path to the folder containing the images for which predictions are to be made.</p> required <code>imageList</code> <code>list</code> <p>List of image filenames for which predictions are to be generated.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping image filenames to their predicted labels.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the model file is not found at the specified path.</p> <code>ValueError</code> <p>If no valid image features are extracted from the input images.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def predictLightgbm(imageFolder, imageList):\n    \"\"\"\n    Predict the labels for a list of images using a pre-trained LightGBM model.\n\n    This function loads a pre-trained LightGBM model and PCA transformation, extracts features for the new images, applies PCA, and makes predictions using the loaded model.\n\n    :param imageFolder: Path to the folder containing the images for which predictions are to be made.\n    :type imageFolder: str\n    :param imageList: List of image filenames for which predictions are to be generated.\n    :type imageList: list\n\n    :return: A dictionary mapping image filenames to their predicted labels.\n    :rtype: dict\n\n    :raises FileNotFoundError: If the model file is not found at the specified path.\n    :raises ValueError: If no valid image features are extracted from the input images.\n    \"\"\"\n    modelPath = os.path.join(processControl.env['models'], \"lightgbm_model.pkl\")\n\n    if not os.path.exists(modelPath):\n        raise FileNotFoundError(f\"Model file not found at {modelPath}\")\n\n    clf = joblib.load(modelPath)\n    log_(\"info\", logger, f\"Model loaded from {modelPath}\")\n\n    # Extract features for new images\n    X_new = extractFeaturesForInference(imageFolder)\n    pca = joblib.load(os.path.join(processControl.env['models'], \"pca_transform.pkl\"))\n    X_new = pca.transform(X_new)\n    assert X_new.shape[1] == processControl.defaults['features'], f\"Expected {processControl.defaults['features']} features, got {X_new.shape[1]}\"\n\n    if X_new.shape[0] == 0:\n        raise ValueError(\"No valid image features extracted. Check your input images.\")\n\n    # Make predictions\n    predictions = clf.predict(X_new)\n    return dict(zip(imageList, predictions))  # Return as dictionary (image -&gt; cluster)\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.predictTransformers","title":"<code>predictTransformers(imageFolder, imageList)</code>","text":"<p>Make predictions using a pre-trained transformer model.</p> <p>This function loads a pre-trained transformer model, extracts features from images in the specified folder, applies PCA transformation to these features, and then uses the model to make predictions on the transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>imageFolder</code> <code>str</code> <p>The folder containing the images for which predictions are to be made.</p> required <code>imageList</code> <code>list</code> <p>A list of image filenames for which predictions are to be made.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The predictions made by the transformer model.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def predictTransformers(imageFolder, imageList):\n    \"\"\"\n    Make predictions using a pre-trained transformer model.\n\n    This function loads a pre-trained transformer model, extracts features from images in the specified folder, applies PCA transformation to these features, and then uses the model to make predictions on the transformed features.\n\n    :param imageFolder: The folder containing the images for which predictions are to be made.\n    :type imageFolder: str\n    :param imageList: A list of image filenames for which predictions are to be made.\n    :type imageList: list\n\n    :return: The predictions made by the transformer model.\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    from transformers import AutoModelForSequenceClassification, Trainer\n    from datasets import Dataset\n    import joblib\n    import numpy as np\n    import os\n\n    modelPath = os.path.join(processControl.env['models'], \"transformers_model\")\n    pcaPath = os.path.join(processControl.env['models'], \"pca_transform.pkl\")\n\n    # Load trained transformer model\n    model = AutoModelForSequenceClassification.from_pretrained(modelPath)\n\n    X_new = extractFeaturesForInference(imageFolder)\n\n    pca = joblib.load(os.path.join(processControl.env['models'], \"pca_transform.pkl\"))\n    X_new = pca.transform(X_new)\n    # Convert to Hugging Face dataset\n    new_dataset = Dataset.from_dict({\"features\": X_new})\n\n    # Load Trainer and make predictions\n    trainer = Trainer(model=model)\n    predictions = trainer.predict(new_dataset)\n\n    return predictions.predictions\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.prepare_data","title":"<code>prepare_data(features_file, labels)</code>","text":"<p>Prepare training and testing data from precomputed features and labels.</p> <p>This function loads image features and corresponding labels from a specified file, splits the data into training and testing sets, and applies oversampling to the training set if needed. SMOTE or RandomOverSampler can be used to balance the class distribution in the training data.</p> <p>Parameters:</p> Name Type Description Default <code>features_file</code> <code>str</code> <p>Path to the file containing the precomputed image features.</p> required <code>labels</code> <code>dict</code> <p>A dictionary mapping image names to their corresponding labels.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the following: - X_train (numpy.ndarray): The feature matrix for the training set. - X_test (numpy.ndarray): The feature matrix for the test set. - y_train (numpy.ndarray): The labels for the training set. - y_test (numpy.ndarray): The labels for the test set.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def prepare_data(features_file, labels):\n    \"\"\"\n    Prepare training and testing data from precomputed features and labels.\n\n    This function loads image features and corresponding labels from a specified file, splits the data into training and testing sets, and applies oversampling to the training set if needed. SMOTE or RandomOverSampler can be used to balance the class distribution in the training data.\n\n    :param features_file: Path to the file containing the precomputed image features.\n    :type features_file: str\n    :param labels: A dictionary mapping image names to their corresponding labels.\n    :type labels: dict\n\n    :return: A tuple containing the following:\n        - X_train (numpy.ndarray): The feature matrix for the training set.\n        - X_test (numpy.ndarray): The feature matrix for the test set.\n        - y_train (numpy.ndarray): The labels for the training set.\n        - y_test (numpy.ndarray): The labels for the test set.\n    :rtype: tuple\n    \"\"\"\n    X, y = loadFeatureslabels(features_file, labels)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    if processControl.defaults['smoteFeatures']:\n        from imblearn.over_sampling import SMOTE, RandomOverSampler\n        ros = RandomOverSampler(random_state=42)\n        X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n        # El dataset es demasiado peque\u00f1o para smote\n        #smote = SMOTE(k_neighbors=2, random_state=42)\n        #X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n        X_train, y_train = X_train_resampled, y_train_resampled\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.processApply","title":"<code>processApply()</code>","text":"<p>Applies the selected model (LightGBM or Transformers) to predict image clusters.</p> <p>This function extracts features from images located in the specified input path, applies the selected model to make predictions, and returns the predicted clusters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping image filenames to their predicted clusters.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def processApply():\n    \"\"\"\n    Applies the selected model (LightGBM or Transformers) to predict image clusters.\n\n    This function extracts features from images located in the specified input path,\n    applies the selected model to make predictions, and returns the predicted clusters.\n\n    :return: A dictionary mapping image filenames to their predicted clusters.\n    :rtype: dict\n    \"\"\"\n    imageFolder = processControl.env['inputPath']\n    imageList = os.listdir(imageFolder)\n    log_(\"info\", logger, f\"Extracting features for {len(imageList)} images with model {processControl.args.model}\")\n    if processControl.args.model == \"lightgbm\":\n        imagesPredicted = predictLightgbm(imageFolder, imageList)\n\n    if processControl.args.model == \"transformers\":\n        imagesPredicted = predictTransformers(imageFolder, imageList)\n\n    return imagesPredicted\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.processTrain","title":"<code>processTrain(featuresFile, imagesLabels)</code>","text":"<p>Train a model based on the specified model type in processControl.</p> <p>This function trains a model using either LightGBM or transformers, depending on the model type specified in <code>processControl.args.model</code>. It prepares the training data, trains the model, and returns the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>featuresFile</code> <code>str</code> <p>Path to the file containing the image features.</p> required <code>imagesLabels</code> <code>dict</code> <p>A dictionary mapping image names to their corresponding labels.</p> required <p>Returns:</p> Type Description <code>model</code> <p>The trained model.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def processTrain(featuresFile, imagesLabels):\n    \"\"\"\n    Train a model based on the specified model type in processControl.\n\n    This function trains a model using either LightGBM or transformers, depending on the model type specified in `processControl.args.model`. It prepares the training data, trains the model, and returns the trained model.\n\n    :param featuresFile: Path to the file containing the image features.\n    :type featuresFile: str\n    :param imagesLabels: A dictionary mapping image names to their corresponding labels.\n    :type imagesLabels: dict\n\n    :return: The trained model.\n    :rtype: model\n    \"\"\"\n    model = None\n\n    if processControl.args.model == \"lightgbm\":\n        X_train, X_test, y_train, y_test = prepare_data(featuresFile, imagesLabels)\n        model = processTrainLightgbm(X_train, X_test, y_train, y_test)\n\n    elif processControl.args.model == \"transformers\":\n        model = processTrainTransformers(featuresFile, imagesLabels)\n    return model\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.processTrainLightgbm","title":"<code>processTrainLightgbm(X_train, X_test, y_train, y_test)</code>","text":"<p>Train a LightGBM model on preprocessed training data and evaluate its performance.</p> <p>This function normalizes the input features using StandardScaler, applies PCA for dimensionality reduction, trains a LightGBM classifier on the transformed features, and evaluates the model on the test set. A classification report is printed to show the model's performance.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>numpy.ndarray</code> <p>Training feature matrix.</p> required <code>X_test</code> <code>numpy.ndarray</code> <p>Testing feature matrix.</p> required <code>y_train</code> <code>numpy.ndarray</code> <p>Training labels.</p> required <code>y_test</code> <code>numpy.ndarray</code> <p>Testing labels.</p> required <p>Returns:</p> Type Description <code>lightgbm.LGBMClassifier</code> <p>The trained LightGBM classifier.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input feature matrices do not have matching numbers of samples.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def processTrainLightgbm(X_train, X_test, y_train, y_test):\n    \"\"\"\n    Train a LightGBM model on preprocessed training data and evaluate its performance.\n\n    This function normalizes the input features using StandardScaler, applies PCA for dimensionality reduction, trains a LightGBM classifier on the transformed features, and evaluates the model on the test set. A classification report is printed to show the model's performance.\n\n    :param X_train: Training feature matrix.\n    :type X_train: numpy.ndarray\n    :param X_test: Testing feature matrix.\n    :type X_test: numpy.ndarray\n    :param y_train: Training labels.\n    :type y_train: numpy.ndarray\n    :param y_test: Testing labels.\n    :type y_test: numpy.ndarray\n\n    :return: The trained LightGBM classifier.\n    :rtype: lightgbm.LGBMClassifier\n\n    :raises ValueError: If the input feature matrices do not have matching numbers of samples.\n    \"\"\"\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import classification_report\n    import lightgbm as lgb\n\n    # Normalizaci\u00f3n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # PCA\n    pca = PCA(n_components=processControl.defaults['features'])\n    X_train = pca.fit_transform(X_train)\n    X_test = pca.transform(X_test)\n    print(\"Train shape before LightGBM:\", X_train.shape)\n    print(\"Test shape before LightGBM:\", X_test.shape)\n\n    # LightGBM con ajuste de hiperpar\u00e1metros\n    clf = lgb.LGBMClassifier(\n        num_leaves=31,\n        min_data_in_leaf=5,\n        learning_rate=0.05,\n        n_estimators=100,\n        is_unbalance=True,\n        random_state=42,\n        force_row_wise=True\n    )\n\n    # Entrenamiento\n    clf.fit(X_train, y_train)\n\n    # Predicci\u00f3n y evaluaci\u00f3n\n    y_pred = clf.predict(X_test)\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    return clf\n</code></pre>"},{"location":"sources/processTrain.html#sources.processTrain.processTrainTransformers","title":"<code>processTrainTransformers(features_file, labels)</code>","text":"<p>Processes feature and label data to train a Transformer-based classification model.</p> <p>This function loads feature and label data, converts them into tensors, and prepares a dataset compatible with Hugging Face's <code>Trainer</code> API. It splits the data into training and testing sets, initializes a BERT-based model, and configures training parameters. The model is trained, evaluated, and saved to a specified directory. Additionally, the function generates plots to visualize validation loss and accuracy per epoch.</p> <p>Parameters:</p> Name Type Description Default <code>features_file</code> <code>str</code> <p>Path to the file containing feature data.</p> required <code>labels</code> <code>list | numpy.ndarray</code> <p>List or array of labels corresponding to the features.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved model directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an issue with data loading, model training, or saving.</p> Source code in <code>sources/processTrain.py</code> <pre><code>def processTrainTransformers(features_file, labels):\n    \"\"\"\n    Processes feature and label data to train a Transformer-based classification model.\n\n    This function loads feature and label data, converts them into tensors, and prepares a dataset\n    compatible with Hugging Face's `Trainer` API. It splits the data into training and testing sets,\n    initializes a BERT-based model, and configures training parameters. The model is trained,\n    evaluated, and saved to a specified directory. Additionally, the function generates plots\n    to visualize validation loss and accuracy per epoch.\n\n    :param features_file: Path to the file containing feature data.\n    :type features_file: str\n    :param labels: List or array of labels corresponding to the features.\n    :type labels: list or numpy.ndarray\n\n    :return: Path to the saved model directory.\n    :rtype: str\n\n    :raises Exception: If there is an issue with data loading, model training, or saving.\n    \"\"\"\n    from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n    from datasets import Dataset\n    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n    import matplotlib.pyplot as plt\n\n    # Cargar datos\n    X, y = loadFeatureslabels(features_file, labels)\n\n    # Convertir X e y a tensores\n    X = np.array(X)  # Convertir la lista de arrays a un solo numpy.ndarray\n    X = torch.tensor(X)  # Convertir a tensor\n\n    # Asegurar la forma correcta de X si es necesario\n    if len(X.shape) == 2:  # Si falta la dimensi\u00f3n de secuencia\n        X = X[:, np.newaxis, :]  # A\u00f1adir dimensi\u00f3n para secuencia\n\n    y = torch.tensor(y)\n\n    # Crear Dataset de Hugging Face\n    dataset = Dataset.from_dict({\n        \"inputs_embeds\": X.tolist(),  # Convertir directamente a listas compatibles\n        \"labels\": y.tolist()\n    })\n\n    # Divisi\u00f3n en entrenamiento y prueba\n    train_test = dataset.train_test_split(test_size=0.2, seed=42)\n    train_dataset, test_dataset = train_test[\"train\"], train_test[\"test\"]\n\n    # Modelo preentrenado\n    num_labels = len(set(y))  # Determinar din\u00e1micamente el n\u00famero de clases\n    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\n    # Configuraci\u00f3n del entrenamiento\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16\n    )\n\n    # Funci\u00f3n personalizada de m\u00e9tricas\n    def compute_metrics(pred):\n        preds = pred.predictions.argmax(-1)\n        precision = precision_score(pred.label_ids, preds, average='weighted')\n        recall = recall_score(pred.label_ids, preds, average='weighted')\n        f1 = f1_score(pred.label_ids, preds, average='weighted')\n        accuracy = accuracy_score(pred.label_ids, preds)\n        return {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1\n        }\n\n    # Entrenador\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    # Entrenar el modelo\n    training_output = trainer.train()\n\n    # Evaluar el modelo\n    metrics = trainer.evaluate()\n    print(metrics)\n\n    # Guardar el modelo\n    modelPath = os.path.join(processControl.env['models'], \"transformers_model\")\n    trainer.save_model(modelPath)\n    log_(\"info\", logger, f\"Model saved to {modelPath}\")\n\n    # Graficar precisi\u00f3n y p\u00e9rdida\n    plt.figure(figsize=(12, 5))\n    epochs = [log['epoch'] for log in trainer.state.log_history if 'epoch' in log]\n    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n    eval_accuracies = [log['eval_accuracy'] for log in trainer.state.log_history if 'eval_accuracy' in log]\n\n    # Aseg\u00farate de que todas las listas tengan la misma longitud\n    min_len = min(len(epochs), len(eval_losses), len(eval_accuracies))\n\n    # Recorta las listas para que tengan la misma longitud\n    epochs = epochs[:min_len]\n    eval_losses = eval_losses[:min_len]\n    eval_accuracies = eval_accuracies[:min_len]\n\n    # Gr\u00e1fica de Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, eval_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Validation Loss per Epoch')\n    plt.legend()\n\n    # Gr\u00e1fica de Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, eval_accuracies, label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Validation Accuracy per Epoch')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return modelPath\n</code></pre>"},{"location":"sources/common/common.html","title":"common functions","text":""},{"location":"sources/common/common.html#sources.common.common","title":"<code>sources.common.common</code>","text":""},{"location":"sources/common/common.html#sources.common.common.ColoredFormatter","title":"<code>ColoredFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>@Desc: Custom formatter to add color to the console output. @Usage: Enhances console log readability by adding color coding.</p> Source code in <code>sources/common/common.py</code> <pre><code>class ColoredFormatter(logging.Formatter):\n    \"\"\"\n    @Desc: Custom formatter to add color to the console output.\n    @Usage: Enhances console log readability by adding color coding.\n    \"\"\"\n    def format(self, record):\n        color = COLORS.get(record.levelname.lower(), COLORS[\"reset\"])\n        message = super().format(record)\n        return f\"{color}{message}{COLORS['reset']}\"\n</code></pre>"},{"location":"sources/common/common.html#sources.common.common.configureLogger","title":"<code>configureLogger(type='log', loggerName='corpusLog')</code>","text":"<p>@Desc: Configures and returns a logger with rotating file and colored console handlers. @Result: Logger instance ready for logging operations.</p> Source code in <code>sources/common/common.py</code> <pre><code>def configureLogger(type=\"log\", loggerName=\"corpusLog\"):\n    \"\"\"\n    @Desc: Configures and returns a logger with rotating file and colored console handlers.\n    @Result: Logger instance ready for logging operations.\n    \"\"\"\n    if type == \"log\":\n        logFilePath = \"./ProcessLog.txt\"\n    elif type == \"proc\":\n        logFilePath = \"./Process.txt\"\n    logger = logging.getLogger(loggerName)\n\n    if not logger.hasHandlers():\n        logger.setLevel(logging.DEBUG)\n\n        # Rotating file handler\n        fileHandler = RotatingFileHandler(logFilePath, maxBytes=5 * 1024 * 1024, backupCount=5)\n        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n        fileHandler.setFormatter(formatter)\n        logger.addHandler(fileHandler)\n\n        # Console handler with colored output\n        if type == \"log\":\n            # Console handler with colored output\n            console_handler = logging.StreamHandler()\n            color_formatter = ColoredFormatter('%(asctime)s [%(levelname)s] %(message)s')\n            console_handler.setFormatter(color_formatter)\n            logger.addHandler(console_handler)\n\n    return logger\n</code></pre>"},{"location":"sources/common/common.html#sources.common.common.log_","title":"<code>log_(logType, logger, msg)</code>","text":"<p>@Desc: Logs a message with specified log type. @Usage: Calls logger method according to logType.</p> Source code in <code>sources/common/common.py</code> <pre><code>def log_(logType, logger, msg):\n    \"\"\"\n    @Desc: Logs a message with specified log type.\n    @Usage: Calls logger method according to logType.\n    \"\"\"\n    if logType == \"info\":\n        logger.info(msg)\n    elif logType == \"warning\":\n        logger.warning(msg)\n    elif logType == \"debug\":\n        logger.debug(msg)\n    elif logType == \"error\":\n        logger.error(msg)\n    elif logType == \"exception\":\n        logger.exception(msg)\n    else:\n        logger.error(\"Invalid log type specified: %s\", logType)\n</code></pre>"},{"location":"sources/common/common.html#sources.common.common.setLogger","title":"<code>setLogger(logger, debug)</code>","text":"<p>@Desc: Sets logger level based on debug value. @Usage: Adjusts the log detail according to the debug level specified.</p> Source code in <code>sources/common/common.py</code> <pre><code>def setLogger(logger, debug):\n    \"\"\"\n    @Desc: Sets logger level based on debug value.\n    @Usage: Adjusts the log detail according to the debug level specified.\n    \"\"\"\n    if debug == 1:\n        logger.setLevel(logging.DEBUG)\n    elif debug == 2:\n        logger.setLevel(logging.INFO)\n</code></pre>"},{"location":"sources/common/paramsManager.html","title":"execution parameters management","text":""},{"location":"sources/common/paramsManager.html#sources.common.paramsManager","title":"<code>sources.common.paramsManager</code>","text":"<p>@Purpose: Handles project-wide parameters @Usage: Functions called by the main process</p>"},{"location":"sources/common/paramsManager.html#sources.common.paramsManager.getConfigs","title":"<code>getConfigs()</code>","text":"<p>@Desc: Load environment settings, arguments, and hyperparameters. @Result: Stores configurations in processControl variables.</p> Source code in <code>sources/common/paramsManager.py</code> <pre><code>def getConfigs():\n    \"\"\"\n    @Desc: Load environment settings, arguments, and hyperparameters.\n    @Result: Stores configurations in processControl variables.\n    \"\"\"\n\n    processControl.env = manageEnv()\n    processControl.args = manageArgs()\n    processControl.defaults = manageDefaults()\n</code></pre>"},{"location":"sources/common/paramsManager.html#sources.common.paramsManager.manageArgs","title":"<code>manageArgs()</code>","text":"<p>@Desc: Parse command-line arguments to configure the process. @Result: Returns parsed arguments as a Namespace object.</p> Source code in <code>sources/common/paramsManager.py</code> <pre><code>def manageArgs():\n    \"\"\"\n    @Desc: Parse command-line arguments to configure the process.\n    @Result: Returns parsed arguments as a Namespace object.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Main process for Corpus handling.\")\n    parser.add_argument('--proc', type=str, help=\"Process type: MODEL, APPLY\", default=\"MODEL\")\n    parser.add_argument('--model', type=str, help=\"lightgbm, transformers\", default=\"transformers\")\n    return parser.parse_args()\n</code></pre>"},{"location":"sources/common/paramsManager.html#sources.common.paramsManager.manageEnv","title":"<code>manageEnv()</code>","text":"<p>@Desc: Defines environment paths and variables. @Result: Returns a dictionary containing environment paths.</p> Source code in <code>sources/common/paramsManager.py</code> <pre><code>def manageEnv():\n    \"\"\"\n    @Desc: Defines environment paths and variables.\n    @Result: Returns a dictionary containing environment paths.\n    \"\"\"\n\n    config = configLoader()\n    environment = config.get_environment()\n\n    env_data = {}\n    for key, value in environment.items():\n        if \"realPath\" in key:\n            env_data[key] = value\n        else:\n            env_data[key] = os.path.join(environment[\"realPath\"], value)\n\n\n    os.makedirs(env_data['.pycache'], exist_ok=True)\n    os.environ['PYTHONPYCACHEPREFIX'] = env_data['.pycache']\n    sys.pycache_prefix = env_data['.pycache']\n    return env_data\n</code></pre>"},{"location":"sources/common/utils.html","title":"common util functions","text":""},{"location":"sources/common/utils.html#sources.common.utils","title":"<code>sources.common.utils</code>","text":""},{"location":"sources/common/utils.html#sources.common.utils.configLoader","title":"<code>configLoader</code>","text":"<p>@Desc: Loads and provides access to JSON configuration data. @Usage: Instantiates with path to config JSON file.</p> Source code in <code>sources/common/utils.py</code> <pre><code>class configLoader:\n    \"\"\"\n    @Desc: Loads and provides access to JSON configuration data.\n    @Usage: Instantiates with path to config JSON file.\n    \"\"\"\n    def __init__(self, config_path='config.json'):\n        self.base_path = os.path.realpath(os.getcwd())\n        realConfigPath = os.path.join(self.base_path, config_path)\n        self.config = self.load_config(realConfigPath)\n\n    def load_config(self, realConfigPath):\n        \"\"\"\n        @Desc: Loads JSON configuration file.\n        @Result: Returns parsed JSON configuration as a dictionary.\n        \"\"\"\n        with open(realConfigPath, 'r') as config_file:\n            return json.load(config_file)\n\n    def get_environment(self):\n        \"\"\"\n        @Desc: Retrieves MongoDB configuration details.\n        @Result: MongoDB configuration data or None if unavailable.\n        \"\"\"\n        environment =  self.config.get(\"environment\", None)\n        environment[\"realPath\"] = self.base_path\n        return environment\n\n    def get_defaults(self):\n        \"\"\"\n        @Desc: Retrieves environment settings from the configuration.\n        @Result: Environment configuration dictionary.\n        \"\"\"\n        return self.config.get(\"defaults\", {})\n</code></pre>"},{"location":"sources/common/utils.html#sources.common.utils.configLoader.get_defaults","title":"<code>get_defaults()</code>","text":"<p>@Desc: Retrieves environment settings from the configuration. @Result: Environment configuration dictionary.</p> Source code in <code>sources/common/utils.py</code> <pre><code>def get_defaults(self):\n    \"\"\"\n    @Desc: Retrieves environment settings from the configuration.\n    @Result: Environment configuration dictionary.\n    \"\"\"\n    return self.config.get(\"defaults\", {})\n</code></pre>"},{"location":"sources/common/utils.html#sources.common.utils.configLoader.get_environment","title":"<code>get_environment()</code>","text":"<p>@Desc: Retrieves MongoDB configuration details. @Result: MongoDB configuration data or None if unavailable.</p> Source code in <code>sources/common/utils.py</code> <pre><code>def get_environment(self):\n    \"\"\"\n    @Desc: Retrieves MongoDB configuration details.\n    @Result: MongoDB configuration data or None if unavailable.\n    \"\"\"\n    environment =  self.config.get(\"environment\", None)\n    environment[\"realPath\"] = self.base_path\n    return environment\n</code></pre>"},{"location":"sources/common/utils.html#sources.common.utils.configLoader.load_config","title":"<code>load_config(realConfigPath)</code>","text":"<p>@Desc: Loads JSON configuration file. @Result: Returns parsed JSON configuration as a dictionary.</p> Source code in <code>sources/common/utils.py</code> <pre><code>def load_config(self, realConfigPath):\n    \"\"\"\n    @Desc: Loads JSON configuration file.\n    @Result: Returns parsed JSON configuration as a dictionary.\n    \"\"\"\n    with open(realConfigPath, 'r') as config_file:\n        return json.load(config_file)\n</code></pre>"},{"location":"sources/common/utils.html#sources.common.utils.dbTimestamp","title":"<code>dbTimestamp()</code>","text":"<p>@Desc: Generates a timestamp formatted as \"YYYYMMDDHHMMSS\". @Result: Formatted timestamp string.</p> Source code in <code>sources/common/utils.py</code> <pre><code>def dbTimestamp():\n    \"\"\"\n    @Desc: Generates a timestamp formatted as \"YYYYMMDDHHMMSS\".\n    @Result: Formatted timestamp string.\n    \"\"\"\n    timestamp = int(time.time())\n    formatted_timestamp = str(time.strftime(\"%Y%m%d%H%M%S\", time.gmtime(timestamp)))\n    return formatted_timestamp\n</code></pre>"},{"location":"sources/common/utils.html#sources.common.utils.mkdir","title":"<code>mkdir(dir_path)</code>","text":"<p>@Desc: Creates directory if it doesn't exist. @Usage: Ensures a directory exists before proceeding with file operations.</p> Source code in <code>sources/common/utils.py</code> <pre><code>def mkdir(dir_path):\n    \"\"\"\n    @Desc: Creates directory if it doesn't exist.\n    @Usage: Ensures a directory exists before proceeding with file operations.\n    \"\"\"\n    if not isdir(dir_path):\n        os.makedirs(dir_path)\n</code></pre>"}]}